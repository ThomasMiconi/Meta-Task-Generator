23 Jan 2023

PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 0.5 PROBAUSEVARREWARDPROB: 0.8
Special States' ranges: [[1, 2]]
Flipping state 0 actions outcome at positions 0 and 2 other than action 0
Transition table:
 [[[0.33333333 0.33333333 0.33333333]
  [0.33333333 0.33333333 0.33333333]]

 [[1.         0.         0.        ]
  [1.         0.         0.        ]]

 [[0.         1.         0.        ]
  [1.         0.         0.        ]]]
Reward rules:
 [[2, -1, -1, 1.0, 1.0], [0, -1, -1, 1.0, 1.0], [1, -1, -1, 1000, 1.0]]
Stimuli:
 [ 0 -1  0]

As it is, this meta-task has the same optimal strategy for all tasks: in state 2 (only one with choice), always choose to go to state 0.

But if the rewards for states 0 and 1 are made lower, then the optimal strategy depends on the (variable) probbability  of reward in  state 1.

2 Feb 2023

PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 0.8 PROBAUSEVARREWARDPROB: 0.8
Special States' ranges: [[0, 1, 2]]
Transition table:
 [[[1.         0.         0.        ]
  [0.         1.         0.        ]]

 [[0.5        0.         0.5       ]
  [0.         0.         1.        ]]

 [[0.33333333 0.33333333 0.33333333]
  [0.         1.         0.        ]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, -1, 1.0, 1.0], [2, -1, 0, 1.0, 1.0], [2, -1, -1, 1000, 1.0]]
Stimuli:
 [1001 1000   -1]

In state 0, action 0 (looping back)  is always a "delay", because State 0has
guaranteed 0 reward, so  must always take action 1. Implies identifying Stim 1001 !

While reward for state 2 is probabilistic with regenerated proba, this seems to have no impact on optimal strategies... In S1 (when seeing Stim 1000) do action 1, when in state 2 (no stim) take action 0.

May be different if rewards for all  three states, or  even just for state 1,
is also probabilistic - regenerated!Then deciding whether to go to either state
must be assessed. 


PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.2
Special States' ranges: [[1, 2]]
Transition table:
 [[[0.         1.         0.        ]
  [0.5        0.5        0.        ]]

 [[0.5        0.5        0.        ]
  [0.16666667 0.         0.83333333]]

 [[0.5        0.         0.5       ]
  [0.33333333 0.33333333 0.33333333]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, -1, 1.0, 1.0], [1, -1, 0, 1.0, 1.0], [0, -1, -1, 1.0, 1.0]]
Stimuli:
 [   1 1001 1000]

 When seeing stim0, always 0; when seeing fixed (state 0): any, I suppose? When
 seeing stim1 (state 2): always 1, which has a higher proba of escaping the
 non-rrewarded state 2. Thus you must identify Stim0 vs Stim1. This is doe as
 soon as you see one, simply by checking if you get a reward or not...  Like
 Harlow? But at least in Harrlow the reward is dependent on your action... 


PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.8
Special States' ranges: [[0, 1, 2]]
Transition table:
 [[[0.  0.5 0.5]
  [0.5 0.5 0. ]]

 [[0.  0.  1. ]
  [0.5 0.  0.5]]

 [[0.  0.  1. ]
  [0.  0.  1. ]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, 0, 1.0, 1.0], [1, -1, -1, 1.0, 1.0], [1, -1, 0, 1.0, 1.0]]
Stimuli:
 [1001 1000    0]

[Wrong, I think:
Only reward is on an action leading to terminal (self-loop) state: state 1 action 0.
Strat: on seeing Stim 0 take action 1 (guarantees  avoiding termination), on Stim 1 take action 0 (then get reward then terminal)
So like Harlow, though reward/punishment is different and entails terminal states...
]
[Update 7 Feb:
Actually State 1 is always rewarded, so optimal is actually get to  state 1 then action 1, which allows a chance of coming back. To get to state 1 without risk of termination you need action 1 from 0. So optimal strat is simply always-1!]


PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.5
Special States' ranges: [[0, 1]]
Transition table:
 [[[0. 1. 0.]
  [0. 1. 0.]]

 [[1. 0. 0.]
  [0. 1. 0.]]

 [[1. 0. 0.]
  [1. 0. 0.]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[2, -1, -1, 1.0, 1.0], [1, -1, -1, 1.0, 1.0], [100, -1, 0, 1.0, 1.0]]
Stimuli:
 [ 1  0 -1]
 Note State 2 is unreachable and always brings back to 0, can be ignored.
 [Wrong, I think:
 Strat: Identify whether Special State is 0 or 1.  If 0: When seeng FixedStim 0, take action 0. If 1: When seeing FixedStim 0, take action 1. (and when seeing Fixed stim 1, whatever the case, always take action 0)
 Essentially a bandit-like task.]
 [Update 7 Feb:
 State 1 is always rewarded no matter what.  Optimal strat is '0  then repeat 1 forever' (initial  0 is in case rewarded state is 0).Would be different if successive rules were additive, rather than replacing.]


3 Feb 2023

PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.2
Special States' ranges: [[0, 2]]
Transition table:
 [[[0.5 0.5 0. ]
  [0.  0.  1. ]]

 [[1.  0.  0. ]
  [1.  0.  0. ]]

 [[0.  1.  0. ]
  [0.  0.  1. ]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[2, -1, 0, 1.0, 1.0], [100, -1, 1, 1.0, 1.0], [100, -1, 1, 1.0, 1.0]]
Stimuli:
 [ 0  0 -1]
[Wrong! Simple.  Strat:If StimFixed0: action 1, if No-Stim (State 2): either 0 (if SS is 0) or 1 (if SS is 1). CCan be determined bytrying action 1 and testing whether rewarded or not.]
[Update 7 Feb: If SS is 2: optimal actions is 1 all the time. If SS is 0: It's clearly  not 1 allthe time, which
is staying in an unrewarded state 2. Repeatin 1-0-(either) all the time
guarantees  ~1/3 rewardper time step, which seems to be the best you can do.] 
[So at least some variation in optimal strategies!]

PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 1.0 PROBAUSEVARREWARDPROB: 0.5
Special States' ranges: [[1, 2]]
Flipping state 0 actions outcome at positions 1 and 2 other than action 1
Transition table:
 [[[0.33333333 0.33333333 0.33333333]
  [0.33333333 0.33333333 0.33333333]]

 [[1.         0.         0.        ]
  [1.         0.         0.        ]]

 [[0.         1.         0.        ]
  [0.16666667 0.83333333 0.        ]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[2, -1, 1, 1.0, 1.0], [2, -1, -1, 0.5, 1.0], [1, -1, -1, 1000, 1.0]]
Stimuli:
 [1000   -1 1001]
Interesting? If Stim1:  0 if Rew0 is high, 1 if Rew0 is low.
Must identify Stim1, and  then Rew0 high/low.


PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.8
Special States' (if any) ranges: [[2, 0]]
Transition table:
 [[[0.         1.         0.        ]
  [0.33333333 0.33333333 0.33333333]]

 [[0.16666667 0.         0.83333333]
  [0.83333333 0.         0.16666667]]

 [[0.         1.         0.        ]
  [0.         1.         0.        ]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, -1, 1.0, 1.0], [100, -1, -1, 1.0, 1.0], [100, -1, 0, 1.0, 1.0]]
Stimuli:
 [ 0  1 -1]
Strat: If StimFixed0 always 0. If FixedStim1: 0 or 1, depending on whether the
special state (which is rewarded) is 2 or 0. Essentially a simple 2-arm
probbabilistic bandit on state 1's actions.


PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.8
Special States' (if any) ranges: [[0, 2, 1]]
Transition table:
 [[[0.         0.5        0.5       ]
  [0.         0.         1.        ]]

 [[0.         1.         0.        ]
  [0.5        0.         0.5       ]]

 [[0.14285714 0.14285714 0.71428571]
  [0.83333333 0.         0.16666667]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, -1, 1.0, 1.0], [100, -1, -1, 1.0, 1.0], [1, -1, 0, 1.0, 1.0]]
Stimuli:
 [1001 1001 1000]
 Optimal strat not obvious! Depends on which of states 0,1,2 is (rewarded)
 special state. Also note S1 in both states 0 and 1, making it hard to know
 which is which - but that turns out not to matter.
 SS = 1: If Stim0 1, if Stim1 0.
 SS = 0: If Stim0 1, if Stim1 0.
 SS =  2: If Stim0... 0, if Stim1: 0.
 Not so interesting. Best action on seeing stim1 is always 0.


PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 1.0 PROBAUSEVARREWARDPROB: 0.8
Special States' (if any) ranges: [[0, 1, 2]]
Transition table:
 [[[1.         0.         0.        ]
  [0.         0.5        0.5       ]]

 [[0.16666667 0.         0.83333333]
  [1.         0.         0.        ]]

 [[0.         0.5        0.5       ]
  [0.33333333 0.33333333 0.33333333]]]
Reward rules (Old state, new state, action taken, probability, value):
 [[1, -1, -1, 1000, 1.0], [1, -1, -1, 0.2, 1.0], [0, -1, 1, 1000, 1.0]]
Stimuli:
 [ 0  0 -1]
If FixedStim0, always 1. If no stim, either 0 or 1 depending on whether Rew R0 is high or low.


5 Feb 2023

With flags!

PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.8 PROBAUSEFLAG: 0.5
Special States' (if any) ranges: [[2, 1, 3]]
Transition table:
 [[[0.  0.  1.  0. ]
  [0.5 0.  0.5 0. ]]

 [[0.  1.  0.  0. ]
  [0.  0.  0.  1. ]]

 [[0.  1.  0.  0. ]
  [0.  1.  0.  0. ]]

 [[0.5 0.  0.  0.5]
  [0.5 0.  0.  0.5]]]
Reward rules (Old state, new state, action taken, probability, value, flag):
 [[100, -1, 0, 1.0, 1.0, -1], [2, -1, 0, 1.0, 1.0, 0], [100, -1, -1, 1.0, 1.0, 0]]
Flag rules (Old state, new state, action taken, new flag value) (being in state 0 sets flag to 0) (may not be used!):
 [[2, -1, 0, 0.0], [100, -1, -1, 1.0]]
Stimuli:
 [   0   -1 1001 1000]
Starting state always 0.
SS (Special state) is 1,2,3. SS is rewarded if flag = 1. Flag = 1 on 2/0 (which also rewards), or on SS.
If SS = 1: optimal strat is action 0 all the time, going from 0 to 2 to 1  and looping there  (in "Heaven")
If SS = 2 or 3, strategy is the same: 0 - 0 - 1, then (you're in 3) choose
either repeatedly until you're out of 3 (and in 0), then start again.
Would be even more  interesting if probailities in 3 were heterogenous - induce a difference between strategies for SS = 2 vs 3 


7 Feb 2023


PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 1.0 PROBAUSEVARREWARDPROB: 0.8 PROBAUSEFLAG: 0.0
Special States' (if any) ranges: [[1, 0]]
Transition table:
 [[[1.         0.         0.        ]
  [0.5        0.         0.5       ]]

 [[1.         0.         0.        ]
  [0.         1.         0.        ]]

 [[0.5        0.5        0.        ]
  [0.16666667 0.         0.83333333]]]
Reward rules (Old state, new state, action taken, probability, value, flag):
 [[100, -1, -1, 1000, 1.0, -1], [100, -1, -1, 1000, 1.0, -1], [1, -1, -1, 1000, 1.0, -1]]
Flag rules (Old state, new state, action taken, new flag value) (being in state 0 sets flag to 0) (may not be used!):
 [[100, -1, -1, 1.0], [0, -1, 0, 1.0]]
Stimuli:
 [  -1 1001 1000]

If SS is 0 with  high reward, and reward for 1 is low, then optimal strategy is just 0  all the time whenthere is no stimulus
If SS is 1, or if reward for 1 >>reward for  SS=0, optimal  strategyis to hit 1 when no stiim, 1 when stim 1000, then  1 all the time (once  you get in 1 you can stay  there indefinitely).
Since the reward probbability  must be determined, this requires exploration!
"randomly generated task 2" in slides.



PROBAUSESPECIALSTATE: 0.0 PROBAUSEPROBABILISTICREWARDS: 1.0 PROBAUSEVARREWARDPROB: 0.8 PROBAUSEFLAG: 0.5
Special States' (if any) ranges: [[3, 2]]
Transition table:
 [[[0.  0.  0.  1. ]
  [0.  0.  0.  1. ]]

 [[0.  0.  0.  1. ]
  [0.  0.  1.  0. ]]

 [[0.  0.  1.  0. ]
  [0.  1.  0.  0. ]]

 [[0.  0.  0.5 0.5]
  [0.5 0.5 0.  0. ]]]
Reward rules (Old state, new state, action taken, probability, value, flag):
 [[3, -1, 1, 1000, 1.0, -1], [2, -1, -1, 1000, 1.0, 0.0]]
Flag rules (Old state, new state, action taken, new flag value) (being in state 0 sets flag to 0) (may not be used!):
 [[1, -1, 0, 1]]
Stimuli:
 [0 2 2 1]


Different strategies depending on reward values for 3/1 and 2... But the flag does not really enter into the optimal strategies...does
it?
If high reward for 2-flag0, low for 3/1: First any actiongets to 3, thenn seeing stim 1 take action 0 (until out of 3), ending in 2. There, (stim 2), just take action 0 forever.


PROBAUSESPECIALSTATE: 0.5 PROBAUSEPROBABILISTICREWARDS: 0.0 PROBAUSEVARREWARDPROB: 0.5 PROBAUSEFLAG: 0.5
Special States' (if any) ranges: [[1, 3, 2]]
Transition table:
 [[[0.         0.5        0.5        0.        ]
  [0.         0.         1.         0.        ]]

 [[0.         0.5        0.5        0.        ]
  [0.         0.         0.         1.        ]]

 [[0.33333333 0.33333333 0.         0.33333333]
  [0.33333333 0.33333333 0.         0.33333333]]

 [[0.         1.         0.         0.        ]
  [0.         1.         0.         0.        ]]]
Reward rules (Old state, new state, action taken, probability, value, flag):
 [[2, -1, -1, 1.0, 1.0, 1.0]]
Flag rules (Old state, new state, action taken, new flag value) (being in state 0 sets flag to 0) (may not be used!):
 [[100, -1, -1, 1]]
Stimuli:
 [1000   -1 1001    1]

A crude  key-door meta-task? Reward for getting to state 2  (door) but only if
flag is set (key is picked); the flag is set by visiting the special state,
which can be any of the non-initial  states. Note that state 2 immediately and
randomly teleports you to any of the non-zero states.

Randomly generated task 3 in the slides.
